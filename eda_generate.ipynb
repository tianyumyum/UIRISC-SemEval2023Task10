{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/B1_1.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "original='/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/B1_1.csv'\n",
    "result_txt=original[:-4]+\".txt\"\n",
    "result_eda_csv=original[:-4]+\"_1eda.csv\"\n",
    "print(result_txt)\n",
    "#print()\n",
    "df = pd.read_csv(original,engine='python')\n",
    "df=df[0:7000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none                                        5614\n",
       "2. derogation                                697\n",
       "3. animosity                                 462\n",
       "4. prejudiced discussions                    123\n",
       "1. threats, plans to harm and incitement     104\n",
       "Name: label_category, dtype: int64"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_category'].value_counts()\n",
    "# 2 含有贬义的言辞(或行为)\n",
    "# 3 仇恨; 愤怒; 敌意; 憎恶;\n",
    "# 4 偏见的讨论\n",
    "# 1 威胁，计划伤害和煽动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('train_all_tasks_eda.csv',engine='python')\n",
    "# df.info()\n",
    "# df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# 将training_all_tasks_new.csv改为EDA所需的txt文本\n",
    "# 数据格式为标签\\t文本\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def change_to_txt(csvname,result_txt):\n",
    "    data = pd.read_csv(csvname, encoding='UTF-8')\n",
    "    #data=data[data['label_category']!='none']\n",
    "    #data=data[:7000]\n",
    "    id = data.iloc[:, 0].tolist()\n",
    "    text = data.iloc[:, 13].tolist()\n",
    "    label_category= data.iloc[:, 3].tolist()\n",
    "    \n",
    "    text_stop=[]\n",
    "    for i in text:\n",
    "        words = i.split(' ')\n",
    "        words = [word for word in words if word != '']\n",
    "        #words=[str(word) for word in words if word not in stopwords.words('english')]\n",
    "        #print(\" \".join([word for word in words if word not in stopwords]))\n",
    "        text_stop.append(\" \".join(words))\n",
    "\n",
    "\n",
    " \n",
    "    label = []\n",
    "    for i in label_category:\n",
    "        # if i == '1. threats, plans to harm and incitement':\n",
    "        #     label.append('1')\n",
    "        # elif i == '2. derogation':\n",
    "        #     label.append('2')\n",
    "        # elif i == '3. animosity':\n",
    "        #     label.append('3')\n",
    "        # elif i == '4. prejudiced discussions':\n",
    "        #     label.append('4')\n",
    "        if i==\"sexist\":\n",
    "            label.append('1')\n",
    "        elif i==\"not sexist\":\n",
    "            label.append('0')\n",
    "\n",
    "    data_all = []\n",
    "    for j in range(len(id)):\n",
    "        #print(j)\n",
    "        #print(label[j])\n",
    "        #print(id[j])\n",
    "        #print(text_stop[j])\n",
    "        a = label[j] +'\\t'+ id[j] + '\\t' + text_stop[j]\n",
    "        data_all.append(a)\n",
    "    f = open(result_txt, 'w', encoding='UTF-8')\n",
    "    for k in data_all:\n",
    "        f.write(k+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei and Kai Zou\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(1)\n",
    "# for the first time you use wordnet\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# stop words list\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
    "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
    "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
    "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
    "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
    "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
    "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
    "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "\t\t\t'should', 'now', '', '[URL]']\n",
    "\n",
    "\n",
    "# cleaning up text\n",
    "import re\n",
    "\n",
    "\n",
    "def get_only_chars(line):\n",
    "\tclean_line = \"\"\n",
    "\tline = line.replace(\"’\", \"\")\n",
    "\tline = line.replace(\"'\", \"\")\n",
    "\tline = line.replace(\"-\", \" \")  #r eplace hyphens with spaces\n",
    "\tline = line.replace(\"\\t\", \" \")\n",
    "\tline = line.replace(\"\\n\", \" \")\n",
    "\tline = line.lower()\n",
    "\tfor char in line:\n",
    "\t\tif char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "\t\t\tclean_line += char\n",
    "\t\telse:\n",
    "\t\t\tclean_line += ' '\n",
    "\tclean_line = re.sub(' +',' ', clean_line)  # delete extra spaces\n",
    "\tif clean_line[0] == ' ':\n",
    "\t\tclean_line = clean_line[1:]\n",
    "\treturn clean_line\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t# print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: # only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t# this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t# obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t# randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t# if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\t\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word != '']\n",
    "\t#print(words)\n",
    "\t#words=[word for word in words if word not in stop_words]\n",
    "\tnum_words = len(words)\n",
    "\t\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4)+1\n",
    "\n",
    "\t# sr\n",
    "\tif (alpha_sr > 0):\n",
    "\t\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t# ri\n",
    "\tif (alpha_ri > 0):\n",
    "\t\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t# rs\n",
    "\tif (alpha_rs > 0):\n",
    "\t\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_swap(words, n_rs)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t# rd\n",
    "\tif (p_rd > 0):\n",
    "\t\tfor _ in range(num_new_per_technique):\n",
    "\t\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\t# trim so that we have the desired number of augmented sentences\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\t# append the original sentence\n",
    "\taugmented_sentences.append(sentence)\n",
    "\treturn augmented_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from eda代码.eda import *\n",
    "import pandas as pd\n",
    "# arguments to be parsed from command line\n",
    "import argparse\n",
    "ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"--input\", required=True, type=str, help=\"input file of unaugmented data\")\n",
    "ap.add_argument(\"--input\", type=str, help=\"input file of unaugmented data\", default=result_txt)\n",
    "ap.add_argument(\"--output\", required=False, type=str, help=\"output file of unaugmented data\")\n",
    "ap.add_argument(\"--num_aug\", required=False, type=int, help=\"number of augmented sentences per original sentence\")\n",
    "ap.add_argument(\"--alpha_sr\", required=False, type=float, help=\"percent of words in each sentence to be replaced by synonyms\")\n",
    "ap.add_argument(\"--alpha_ri\", required=False, type=float, help=\"percent of words in each sentence to be inserted\")\n",
    "ap.add_argument(\"--alpha_rs\", required=False, type=float, help=\"percent of words in each sentence to be swapped\")\n",
    "ap.add_argument(\"--alpha_rd\", required=False, type=float, help=\"percent of words in each sentence to be deleted\")\n",
    "#args = ap.parse_args()\n",
    "args = ap.parse_args(args=[])\n",
    "\n",
    "# the output file\n",
    "output = None\n",
    "if args.output:\n",
    "    output = args.output\n",
    "else:\n",
    "    from os.path import dirname, basename, join\n",
    "    output = join(dirname(args.input), 'eda_' + basename(args.input))\n",
    "\n",
    "# number of augmented sentences to generate per original sentence\n",
    "# num_aug = 9  # default\n",
    "num_aug = 6  # 每条数据再扩充4条\n",
    "if args.num_aug:\n",
    "    num_aug = args.num_aug\n",
    "\n",
    "# how much to replace each word by synonyms\n",
    "alpha_sr = 0.1  # default\n",
    "if args.alpha_sr is not None:\n",
    "    alpha_sr = args.alpha_sr\n",
    "\n",
    "# how much to insert new words that are synonyms\n",
    "alpha_ri = 0.1  # default\n",
    "if args.alpha_ri is not None:\n",
    "    alpha_ri = args.alpha_ri\n",
    "\n",
    "# how much to swap words\n",
    "alpha_rs = 0.1  # default\n",
    "if args.alpha_rs is not None:\n",
    "    alpha_rs = args.alpha_rs\n",
    "\n",
    "# how much to delete words\n",
    "alpha_rd = 0  # default=0.1\n",
    "if args.alpha_rd is not None:\n",
    "    alpha_rd = args.alpha_rd\n",
    "\n",
    "if alpha_sr == alpha_ri == alpha_rs == alpha_rd == 0:\n",
    "     ap.error('At least one alpha should be greater than zero')\n",
    "\n",
    "\n",
    "# # generate more data with standard augmentation\n",
    "def gen_eda(train_orig, output_file, alpha_sr, alpha_ri, alpha_rs, alpha_rd, num_aug):\n",
    "\n",
    "    writer = open(output_file, 'w', encoding='UTF-8')\n",
    "    lines = open(train_orig, 'r', encoding='UTF-8').readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line[:-1].split('\\t')\n",
    "        id=parts[1]\n",
    "        label = parts[0]\n",
    "        sentence = parts[2]\n",
    "        aug_sentences = eda(sentence, alpha_sr=alpha_sr, alpha_ri=alpha_ri, alpha_rs=alpha_rs, p_rd=alpha_rd, num_aug=num_aug)\n",
    "        for aug_sentence in aug_sentences:\n",
    "            writer.write(label+\"\\t\" + aug_sentence + '\\t' + id + \"\\n\")\n",
    "\n",
    "    writer.close()\n",
    "    print(\"generated augmented sentences with eda for \" + train_orig + \" to \" + output_file + \" with num_aug=\" + str(num_aug))\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def txt_adjust(txt):\n",
    "    # 将EDA后的txt文件转换为csv，需对应修改label及输出文件名\n",
    "    file = open(txt, 'r', encoding='UTF-8')\n",
    "    data = file.readlines()\n",
    "    all = []\n",
    "    label = []\n",
    "    text = []\n",
    "    id=[]\n",
    "    # label = []\n",
    "    for line in data:\n",
    "        f = line.split('\\t')  # 将每行数据按\\t重新划分为标签和数据\n",
    "        all.append(f)\n",
    "    for i in all:\n",
    "        #print(i)\n",
    "        #print(i[1])\n",
    "        i[1] = i[1].replace('\\n', '')\n",
    "        i[2] = i[2].replace('\\n', '')\n",
    "        label.append(i[0])\n",
    "        text.append(i[1])\n",
    "        id.append(i[2])\n",
    "    file.close()\n",
    "    df = pd.DataFrame()\n",
    "    df['label'] = label\n",
    "    df['text'] = text\n",
    "    df['rewire_id'] = id\n",
    "    # df['label'] = label\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated augmented sentences with eda for /Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/B1_1.txt to /Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/eda_B1_1.txt with num_aug=6\n"
     ]
    }
   ],
   "source": [
    "# main function\n",
    "if __name__ == \"__main__\":\n",
    "    # 在configuration的parameter中添加 --input=需要数据增强的txt文件名\n",
    "    # generate augmented sentences and output into a new file\n",
    "    change_to_txt(original,result_txt)\n",
    "    txt = gen_eda(args.input, output, alpha_sr=alpha_sr, alpha_ri=alpha_ri,\n",
    "                  alpha_rs=alpha_rs, alpha_rd=alpha_rd, num_aug=num_aug)\n",
    "    file = txt_adjust(txt)\n",
    "    file.to_csv(result_eda_csv, index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13332 entries, 0 to 13331\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   label      13332 non-null  int64 \n",
      " 1   text       13332 non-null  object\n",
      " 2   rewire_id  13332 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 312.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 108892 entries, 0 to 108891\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   label      108892 non-null  int64 \n",
      " 1   text       108892 non-null  object\n",
      " 2   rewire_id  108892 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_0=pd.read_csv(\"/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/B0_1_eda.csv\",engine='python')\n",
    "df_0.info()\n",
    "df_1=pd.read_csv(\"/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/B1_1_1eda.csv\",engine='python')\n",
    "df_1.info()\n",
    "# df_2=pd.read_csv(\"/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/B1_1_2eda.csv\",engine='python')\n",
    "# df_2.info()\n",
    "h= pd.concat([df_0,df_1],axis=0)\n",
    "h.to_csv(\"/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/gold_4_6.csv\", index=False, encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 85400 entries, 0 to 122216\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   label      85400 non-null  int64 \n",
      " 1   text       85400 non-null  object\n",
      " 2   rewire_id  85400 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_14=pd.read_csv('/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/C/train_all_tasks.csv',engine='python')\n",
    "df = pd.read_csv('/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/gold_4_6.csv',engine='python')\n",
    "\n",
    "file1=np.array(df_14)\n",
    "x=[x[0] for x in file1]\n",
    "\n",
    "df=df[df[\"rewire_id\"].isin(x)]\n",
    "df.info()\n",
    "df.to_csv(\"/Users/tianyumi/Desktop/Projects/EDOS/Edos_Coding/EDA/gold/gold_3_6.csv\", index=False, encoding='UTF-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8dfb4c0b3f6ac3a64d7a4572975dd3eb4bfd4b5491ce12660c6428802afd775b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
