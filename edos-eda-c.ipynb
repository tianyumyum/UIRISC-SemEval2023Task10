{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1895ea70",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-09T04:26:07.117847Z",
     "iopub.status.busy": "2023-01-09T04:26:07.117179Z",
     "iopub.status.idle": "2023-01-09T04:26:07.127479Z",
     "shell.execute_reply": "2023-01-09T04:26:07.126697Z"
    },
    "papermill": {
     "duration": 0.020001,
     "end_time": "2023-01-09T04:26:07.129553",
     "exception": false,
     "start_time": "2023-01-09T04:26:07.109552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install transformers --upgrade\n",
    "# !pip install datasets\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install scipy\n",
    "# !pip install sklearn\n",
    "#!pip install scikit-learn\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1928a4c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T04:26:07.141390Z",
     "iopub.status.busy": "2023-01-09T04:26:07.140802Z",
     "iopub.status.idle": "2023-01-09T04:26:10.537362Z",
     "shell.execute_reply": "2023-01-09T04:26:10.535123Z"
    },
    "papermill": {
     "duration": 3.404423,
     "end_time": "2023-01-09T04:26:10.539216",
     "exception": true,
     "start_time": "2023-01-09T04:26:07.134793",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ErnieModel' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23/1500393940.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBertConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mErnieModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ErnieModel' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertModel,BertConfig\n",
    "from transformers import ErnieModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import logging\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf969181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:54.828629Z",
     "iopub.status.busy": "2023-01-09T03:56:54.828236Z",
     "iopub.status.idle": "2023-01-09T03:56:54.835519Z",
     "shell.execute_reply": "2023-01-09T03:56:54.834493Z",
     "shell.execute_reply.started": "2023-01-09T03:56:54.828531Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag=4\n",
    "num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727aa86b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:54.852189Z",
     "iopub.status.busy": "2023-01-09T03:56:54.851937Z",
     "iopub.status.idle": "2023-01-09T03:56:54.971240Z",
     "shell.execute_reply": "2023-01-09T03:56:54.970101Z",
     "shell.execute_reply.started": "2023-01-09T03:56:54.852166Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/edos-dev-task/training.csv',engine='python',names=['rewire_id','text','label_sexist','label_category','label_vector','label_sexist0','sentiment'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7929725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:54.974277Z",
     "iopub.status.busy": "2023-01-09T03:56:54.973861Z",
     "iopub.status.idle": "2023-01-09T03:56:54.985192Z",
     "shell.execute_reply": "2023-01-09T03:56:54.983796Z",
     "shell.execute_reply.started": "2023-01-09T03:56:54.974225Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['label_vector'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d62cce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:54.987956Z",
     "iopub.status.busy": "2023-01-09T03:56:54.986697Z",
     "iopub.status.idle": "2023-01-09T03:56:55.032543Z",
     "shell.execute_reply": "2023-01-09T03:56:55.031601Z",
     "shell.execute_reply.started": "2023-01-09T03:56:54.987919Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df['label_vector']=='1.1 threats of harm','c_1']=0\n",
    "df.loc[df['label_vector']=='1.2 incitement and encouragement of harm','c_1']=1\n",
    "#df.loc[df['label_vector']=='none','c_1']=100\n",
    "\n",
    "df.loc[df['label_vector']=='2.1 descriptive attacks','c_2']=0 #717 2.2 0.176\n",
    "df.loc[df['label_vector']=='2.2 aggressive and emotive attacks','c_2']=1 #673 2.2 0.176\n",
    "df.loc[df['label_vector']=='2.3 dehumanising attacks & overt sexual objectification','c_2']=2 # 200 8 0.64\n",
    "#df.loc[df['label_vector']=='none','c_2']=100\n",
    "\n",
    "df.loc[df['label_vector']=='3.1 casual use of gendered slurs, profanities, and insults','c_3']=0\n",
    "df.loc[df['label_vector']=='3.2 immutable gender differences and gender stereotypes','c_3']=1\n",
    "df.loc[df['label_vector']=='3.3 backhanded gendered compliments','c_3']=2\n",
    "df.loc[df['label_vector']=='3.4 condescending explanations or unwelcome advice','c_3']=3\n",
    "# 637,417,64,47\n",
    "0.1,0.15,0.35, 0.4\n",
    "\n",
    "#df.loc[df['label_vector']=='none','c_3']=100\n",
    "\n",
    "df.loc[df['label_vector']=='4.1 supporting mistreatment of individual women','c_4']=0\n",
    "df.loc[df['label_vector']=='4.2 supporting systemic discrimination against women as a group','c_4']=1\n",
    "#df.loc[df['label_vector']=='none','c_4']=100\n",
    "# 75 258\n",
    "# 0.22 0.78\n",
    "\n",
    "df.fillna(value=100)\n",
    "\n",
    "df.info()\n",
    "#print(df)\n",
    "#df.to_csv(\"/kaggle/working/testfile_see2.csv\",index=False) #save to file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6882f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:55.034505Z",
     "iopub.status.busy": "2023-01-09T03:56:55.034094Z",
     "iopub.status.idle": "2023-01-09T03:56:55.047667Z",
     "shell.execute_reply": "2023-01-09T03:56:55.046551Z",
     "shell.execute_reply.started": "2023-01-09T03:56:55.034462Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##设置当前的任务为：1 2 3 4 \n",
    "##分别对应 7 8 9 10\n",
    "print(df['c_1'].value_counts())\n",
    "df['c_2'].value_counts()\n",
    "df['c_3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ff634",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:55.051503Z",
     "iopub.status.busy": "2023-01-09T03:56:55.050778Z",
     "iopub.status.idle": "2023-01-09T03:56:55.079427Z",
     "shell.execute_reply": "2023-01-09T03:56:55.078521Z",
     "shell.execute_reply.started": "2023-01-09T03:56:55.051465Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = df.fillna(\n",
    "    100,  # nan的替换值\n",
    "    inplace=False  # 是否跟换源文件\n",
    ")\n",
    "\n",
    "file1=np.array(df2)\n",
    "\n",
    "data=[]\n",
    "for item in file1:\n",
    "    if item[tag+6]!=100:\n",
    "        item[tag+6]=item[tag+6]#label\n",
    "        item[1]=str(item[1])#text\n",
    "        item[0]=item[0][19:]\n",
    "        #print(item[1],item[7],item[0])\n",
    "        data.append(list(item))\n",
    "\n",
    "x=[x[1] for x in data]#text\n",
    "y=[x[tag+6] for x in data]#label\n",
    "z=[x[0] for x in data]#id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d395b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:55.081265Z",
     "iopub.status.busy": "2023-01-09T03:56:55.080902Z",
     "iopub.status.idle": "2023-01-09T03:56:55.088649Z",
     "shell.execute_reply": "2023-01-09T03:56:55.087662Z",
     "shell.execute_reply.started": "2023-01-09T03:56:55.081232Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, z_train, z_test =  train_test_split(x, y, z, test_size=0.1,train_size=0.9)\n",
    "#z_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2f992",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:55.091784Z",
     "iopub.status.busy": "2023-01-09T03:56:55.091168Z",
     "iopub.status.idle": "2023-01-09T03:56:55.133549Z",
     "shell.execute_reply": "2023-01-09T03:56:55.132390Z",
     "shell.execute_reply.started": "2023-01-09T03:56:55.091738Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_dev=pd.read_csv('/kaggle/input/edos-dev-task/dev_task_c_entries.csv')\n",
    "df_dev.info()\n",
    "\n",
    "df_re=pd.read_csv('/kaggle/input/edosbtest/testfile_large_63.5.csv')\n",
    "df_re.info()\n",
    "df_data=pd.merge(df_dev,df_re,on='rewire_id')\n",
    "df_data.info()\n",
    "x_dev= df_data['text'].tolist()\n",
    "\n",
    "df_data2=np.array(df_data)\n",
    "#z_dev=[int(x[19:]) for x in list(df_dev['rewire_id']) if x[2][0]==tag]\n",
    "#z_dev=[x[0] for x in list(df_data['label_pred']) if int(x[0])==tag]\n",
    "z_dev=[x[0][19:] for x in df_data2 if int(x[2][0])==tag]\n",
    "print(z_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47616c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:55.135518Z",
     "iopub.status.busy": "2023-01-09T03:56:55.135156Z",
     "iopub.status.idle": "2023-01-09T03:56:56.347280Z",
     "shell.execute_reply": "2023-01-09T03:56:56.346266Z",
     "shell.execute_reply.started": "2023-01-09T03:56:55.135483Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
    "train_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=50)\n",
    "test_encoding = tokenizer(x_test, truncation=True, padding=True, max_length=50)\n",
    "dev_encoding=tokenizer(x_dev,truncation=True,padding=True, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb51905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:56.349122Z",
     "iopub.status.busy": "2023-01-09T03:56:56.348720Z",
     "iopub.status.idle": "2023-01-09T03:56:56.355389Z",
     "shell.execute_reply": "2023-01-09T03:56:56.354257Z",
     "shell.execute_reply.started": "2023-01-09T03:56:56.349080Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_encoding.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a5e36",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 数据集封装和处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d2821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:56.360337Z",
     "iopub.status.busy": "2023-01-09T03:56:56.359620Z",
     "iopub.status.idle": "2023-01-09T03:56:56.373202Z",
     "shell.execute_reply": "2023-01-09T03:56:56.372253Z",
     "shell.execute_reply.started": "2023-01-09T03:56:56.360301Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 数据集读取, 继承torch的Dataset类，方便后面用DataLoader封装数据集\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels,ids):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.ids=ids\n",
    "    \n",
    "    #这里的idx是为了让后面的DataLoader成批处理成迭代器，按idx映射到对应数据\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.as_tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.as_tensor(int(self.labels[idx]))\n",
    "        item['ids']=torch.as_tensor(int(self.ids[idx]))\n",
    "        return item\n",
    "    \n",
    "    #数据集长度。通过len(这个实例对象)，可以查看长度\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class DevDataset(Dataset):\n",
    "    def __init__(self, encodings,ids):\n",
    "        self.encodings = encodings\n",
    "        self.ids=ids\n",
    "    \n",
    "    #这里的idx是为了让后面的DataLoader成批处理成迭代器，按idx映射到对应数据\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.as_tensor(val[idx]) for key,val in self.encodings.items()}\n",
    "         #item['encodings']=torch.as_tensor(self.encodings[idx])\n",
    "        item['ids']=torch.as_tensor(int(self.ids[idx]))\n",
    "        return item\n",
    "    \n",
    "    #数据集长度。通过len(这个实例对象)，可以查看长度\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "#将数据集包装成torch的Dataset形式\n",
    "train_dataset = NewsDataset(train_encoding, y_train,z_train)\n",
    "test_dataset = NewsDataset(test_encoding, y_test,z_test)\n",
    "dev_dataset=DevDataset(dev_encoding,z_dev)\n",
    "# 单个读取到批量读取\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "dev_dataloader=DataLoader(dev_dataset,batch_size=16,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2d737",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:56.377004Z",
     "iopub.status.busy": "2023-01-09T03:56:56.376696Z",
     "iopub.status.idle": "2023-01-09T03:56:56.386851Z",
     "shell.execute_reply": "2023-01-09T03:56:56.385938Z",
     "shell.execute_reply.started": "2023-01-09T03:56:56.376958Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,num_classes,checkpoint=\"bert-large-uncased\"):\n",
    "        super().__init__()\n",
    "        self.pretrained = ErnieModel.from_pretrained(\"nghuyong/ernie-2.0-large-en\")\n",
    "        self.fc = torch.nn.Sequential(torch.nn.Linear(1024, num_classes))\n",
    " \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        logits = self.pretrained(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = logits.last_hidden_state[:, 0]\n",
    "        logits = self.fc(logits)\n",
    "        return logits\n",
    "    \n",
    "# class Config(object):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         self.pre_bert_path=\"nghuyong/ernie-1.0\"\n",
    "#         self.train_path = 'data/dataset_train.csv'  # 训练集\n",
    "#         self.dev_path = 'data/dataset_valid.csv'  # 验证集\n",
    "#         self.test_path = 'data/test.csv'  # 测试集\n",
    "#         self.class_path = 'data/class.json'  # 类别名单\n",
    "#         self.save_path ='mymodel/ernie.pth'        # 模型训练结果\n",
    "#         self.num_classes=10\n",
    "#         self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # 设备\n",
    "\n",
    "#         self.epochs = 10  # epoch数\n",
    "#         self.batch_size = 128  # mini-batch大小\n",
    "#         self.maxlen = 32  # 每句话处理成的长度(短填长切)\n",
    "#         self.learning_rate = 5e-4                                       # 学习率\n",
    "#         self.hidden_size=768\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(self.pre_bert_path)\n",
    "\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super(Model, self).__init__()\n",
    "#         self.ernie=AutoModel.from_pretrained(config.pre_bert_path)\n",
    "#         #设置不更新预训练模型的参数\n",
    "#         for param in self.ernie.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         self.fc = nn.Linear(config.hidden_size, config.num_classes)\n",
    "#     def forward(self, input):\n",
    "#         out=self.ernie(input_ids =input['input_ids'],attention_mask=input['attention_mask'],token_type_ids=input['token_type_ids'])\n",
    "#         #只取最后一层CLS对应的输出\n",
    "#         out = self.fc(out.pooler_output)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e9f9c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 实例化模型、定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb91ce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:56:56.388989Z",
     "iopub.status.busy": "2023-01-09T03:56:56.388496Z",
     "iopub.status.idle": "2023-01-09T03:57:06.444789Z",
     "shell.execute_reply": "2023-01-09T03:57:06.443720Z",
     "shell.execute_reply.started": "2023-01-09T03:56:56.388947Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device,'能用')\n",
    "\n",
    "model = Model(num_classes).to(device)\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    #def __init__(self, class_num, alpha=[0.78,0.2], gamma=2, use_alpha=True, size_average=True):#0.6+\n",
    "    #def __init__(self, class_num, alpha=[0.18,0.22,0.7], gamma=2, use_alpha=True, size_average=True):#0.609\n",
    "    #def __init__(self, class_num, alpha=[0.08,0.13,0.37,0.42], gamma=2, use_alpha=True, size_average=True):#0.88\n",
    "    def __init__(self, class_num, alpha=[0.74,0.26], gamma=2, use_alpha=True, size_average=True):#0.91 0.76\n",
    "\n",
    "\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.class_num = class_num\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        if use_alpha:\n",
    "            self.alpha = torch.tensor(alpha).cuda()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.use_alpha = use_alpha\n",
    "        self.size_average = size_average\n",
    "    def forward(self, pred, target):\n",
    "        prob = self.softmax(pred.view(-1,self.class_num))\n",
    "        prob = prob.clamp(min=0.0001,max=1.0)\n",
    "        target_ = torch.zeros(target.size(0),self.class_num).cuda()\n",
    "        target_.scatter_(1, target.view(-1, 1).long(), 1.)\n",
    "        if self.use_alpha:\n",
    "            batch_loss = - self.alpha.double() * torch.pow(1-prob,self.gamma).double() * prob.log().double() * target_.double()\n",
    "        else:\n",
    "            batch_loss = - torch.pow(1-prob,self.gamma).double() * prob.log().double() * target_.double()\n",
    "        batch_loss = batch_loss.sum(dim=1)\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss.sum()\n",
    "        return loss\n",
    "\n",
    "criterion = FocalLoss(class_num=num_classes)\n",
    "    \n",
    "# 优化方法\n",
    "#过滤掉被冻结的参数，反向传播需要更新的参数\n",
    "optim = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "total_steps = len(train_loader) * 1\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95c7d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d0e573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:57:06.447279Z",
     "iopub.status.busy": "2023-01-09T03:57:06.446812Z",
     "iopub.status.idle": "2023-01-09T03:57:06.458455Z",
     "shell.execute_reply": "2023-01-09T03:57:06.456862Z",
     "shell.execute_reply.started": "2023-01-09T03:57:06.447242Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def calculate_f1(stacked,num_classes):\n",
    "    stacked=torch.as_tensor(stacked,dtype=torch.int64)\n",
    "    cmt=torch.zeros(num_classes,num_classes,dtype=torch.int32)\n",
    "    #cmt\n",
    "    for p in stacked:\n",
    "        tl,pl=p.tolist()\n",
    "        cmt[tl,pl]=cmt[tl,pl]+1\n",
    "    M=cmt\n",
    "    n = len(M)\n",
    "    pre_all=[]\n",
    "    rec_all=[]\n",
    "    f1_all=[]\n",
    "    sum_acc=0\n",
    "    for i in range(len(M[0])):\n",
    "        rowsum, colsum = sum(M[i]), sum(M[r][i] for r in range(n))\n",
    "        try:\n",
    "            precision=float(M[i][i]/float(colsum))\n",
    "            pre_all.append(precision)\n",
    "            print(float(M[i][i]))\n",
    "            print(float(colsum))\n",
    "            recall=float(M[i][i]/float(rowsum))\n",
    "            rec_all.append(recall)\n",
    "            f1=2*(precision*recall)/(precision+recall)\n",
    "            f1_all.append(f1)\n",
    "\n",
    "            sum_acc+=M[i][i].item()\n",
    "        except ZeroDivisionError:\n",
    "            print('precision: %s' % 0, 'recall: %s' % 0)\n",
    "    print(cmt)\n",
    "    accuracy=sum_acc/((sum(M).sum()).item())\n",
    "    print('accuracy:%s' % accuracy ,'precision: %s' % (mean(pre_all)), 'recall: %s' % (mean(rec_all)),'F1: %s ' % (mean(f1_all)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecd978",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:57:06.461182Z",
     "iopub.status.busy": "2023-01-09T03:57:06.460698Z",
     "iopub.status.idle": "2023-01-09T03:57:06.475210Z",
     "shell.execute_reply": "2023-01-09T03:57:06.474219Z",
     "shell.execute_reply.started": "2023-01-09T03:57:06.461148Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_csv(stacked,tag):\n",
    "    stacked=torch.as_tensor(stacked).cpu()\n",
    "    t_np = stacked.numpy() #convert to Numpy array\n",
    "    df = pd.DataFrame(t_np) #convert to a dataframe\n",
    "    \n",
    "    df.columns=['rewire_id', 'label_pred']\n",
    "    df['rewire_id']=['sexism2022_english-'+str(x) for x in df['rewire_id']]\n",
    "    if tag==1:\n",
    "        df.loc[df['label_pred']==0,'label_pred']='1.1 threats of harm'\n",
    "        df.loc[df['label_pred']==1,'label_pred']='1.2 incitement and encouragement of harm'\n",
    "        print(df)\n",
    "        df.to_csv(\"/kaggle/working/testfile_c_1.csv\",index=False) #save to file\n",
    "    elif tag==2:\n",
    "        df.loc[df['label_pred']==0,'label_pred']='2.1 descriptive attacks'\n",
    "        df.loc[df['label_pred']==1,'label_pred']='2.2 aggressive and emotive attacks'\n",
    "        df.loc[df['label_pred']==2,'label_pred']='2.3 dehumanising attacks & overt sexual objectification'\n",
    "        df.to_csv(\"/kaggle/working/testfile_c_2.csv\",index=False) #save to file\n",
    "    elif tag==3:\n",
    "        df.loc[df['label_pred']==0,'label_pred']='3.1 casual use of gendered slurs, profanities, and insults'\n",
    "        df.loc[df['label_pred']==1,'label_pred']='3.2 immutable gender differences and gender stereotypes'\n",
    "        df.loc[df['label_pred']==2,'label_pred']='3.3 backhanded gendered compliments'\n",
    "        df.loc[df['label_pred']==3,'label_pred']='3.4 condescending explanations or unwelcome advice'\n",
    "        df.to_csv(\"/kaggle/working/testfile_c_3.csv\",index=False) #save to file\n",
    "    elif tag==4:\n",
    "        df.loc[df['label_pred']==0,'label_pred']='4.1 supporting mistreatment of individual women'\n",
    "        df.loc[df['label_pred']==1,'label_pred']='4.2 supporting systemic discrimination against women as a group'\n",
    "        df.to_csv(\"/kaggle/working/testfile_c_4.csv\",index=False) #save to file\n",
    "\n",
    "    \n",
    "    print(\"保存成功！\")\n",
    "    #Then, to reload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc02c4b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T04:21:27.676959Z",
     "iopub.status.busy": "2023-01-09T04:21:27.676556Z",
     "iopub.status.idle": "2023-01-09T04:21:27.696745Z",
     "shell.execute_reply": "2023-01-09T04:21:27.695688Z",
     "shell.execute_reply.started": "2023-01-09T04:21:27.676926Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = pd.read_csv(\"/kaggle/working/testfile_c_1.csv\")\n",
    "for i in range(2,5):\n",
    "    print(i)\n",
    "    h1 = pd.read_csv(\"/kaggle/working/testfile_c_{}.csv\".format(i))\n",
    "    h= pd.concat([h,h1],axis=0)\n",
    "    print(len(h))\n",
    "h.to_csv(\"/kaggle/working/testfile_c_all.csv\",index=False) #save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677c4f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:57:06.478619Z",
     "iopub.status.busy": "2023-01-09T03:57:06.477569Z",
     "iopub.status.idle": "2023-01-09T03:57:06.489752Z",
     "shell.execute_reply": "2023-01-09T03:57:06.488766Z",
     "shell.execute_reply.started": "2023-01-09T03:57:06.478561Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 精度计算\n",
    "#imports untils\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score\n",
    "batch_size=16\n",
    "def validation():\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        results=torch.zeros(0).to(device)\n",
    "        label_all=torch.zeros(0).to(device)\n",
    "        id_all=torch.zeros(0).to(device)\n",
    "        for batch in test_dataloader:\n",
    "            # 正常传播\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            ids=batch['ids'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = outputs\n",
    "            \n",
    "            # 获取预测结果\n",
    "            preds = outputs.argmax(1).to(device)\n",
    "            results=torch.cat((results,preds),dim=0)#预测的结果\n",
    "            \n",
    "            label_all=torch.cat((label_all,labels),dim=0)\n",
    "            id_all=torch.cat((id_all,ids),dim=0)\n",
    "            \n",
    "        stacked=torch.stack(\n",
    "                (\n",
    "                    torch.as_tensor(label_all).to(device),\n",
    "                    torch.as_tensor(list(map(int,results))).to(device)\n",
    "                )\n",
    "            )\n",
    "        stacked=stacked.t()\n",
    "        calculate_f1(stacked,num_classes)\n",
    "        \n",
    "#         stacked_saved=torch.stack(\n",
    "#                     (\n",
    "#                         torch.as_tensor(list(map(int,id_all))).to(device),\n",
    "#                         torch.as_tensor(list(map(int,label_all))).to(device),#真实结果\n",
    "#                         torch.as_tensor(list(map(int,results))).to(device)#预测结果\n",
    "#                     )\n",
    "#                 )\n",
    "#         stacked_saved=stacked_saved.t()\n",
    "\n",
    "\n",
    "        #print(stacked_saved)\n",
    "    print(\"Average testing loss: %.4f\"%(total_eval_loss/len(test_dataloader)))\n",
    "    print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031d783",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:57:06.491720Z",
     "iopub.status.busy": "2023-01-09T03:57:06.491358Z",
     "iopub.status.idle": "2023-01-09T03:57:06.508754Z",
     "shell.execute_reply": "2023-01-09T03:57:06.507551Z",
     "shell.execute_reply.started": "2023-01-09T03:57:06.491687Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(tag):\n",
    "    batch_size=16\n",
    "    for epoch in range(2):\n",
    "        print(\"------------Epoch: %d ----------------\" % epoch)\n",
    "        train_loss = 0.0 # 训练损失\n",
    "        val_loss = 0.0 # 验证损失\n",
    "        model.train() # 声明开始训练\n",
    "        total_train_loss = 0\n",
    "        iter_num = 0\n",
    "        total_iter = len(train_loader)\n",
    "        \n",
    "        results=torch.zeros(0).to(device)\n",
    "        label_all=torch.zeros(0).to(device)\n",
    "        labels=torch.zeros(0).to(device)\n",
    "        for batch in train_loader:\n",
    "            # 正向传播\n",
    "            optim.zero_grad()\n",
    "            #print(batch)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            ids=batch['ids'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = criterion(outputs, labels)    \n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()# 反向传播\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)   #梯度裁剪，防止梯度爆炸  \n",
    "            \n",
    "            # 获取预测结果\n",
    "            preds = outputs.argmax(1).to(device)\n",
    "            \n",
    "            \n",
    "            stacked=torch.stack(\n",
    "                    (\n",
    "                        ids,\n",
    "                        labels,\n",
    "                        preds\n",
    "                    )\n",
    "                )\n",
    "            stacked=stacked.t()\n",
    "            \n",
    "\n",
    "            # 参数更新\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            iter_num += 1\n",
    "            if(iter_num % 100==0):\n",
    "                print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (epoch, iter_num, loss.item(), iter_num/total_iter*100))\n",
    "        print(\"Epoch: %d, Average training loss: %.4f\"%(epoch, total_train_loss/len(train_loader)))     \n",
    "                \n",
    "        validation()\n",
    "        \n",
    "        model.eval()\n",
    "        print(\"--------------开始分类dev集-----------------\")\n",
    "        with torch.no_grad():\n",
    "            results=torch.zeros(0).to(device)\n",
    "            #label_all=torch.zeros(0).to(device)\n",
    "            ids_all=torch.zeros(0).to(device)\n",
    "            for batch in dev_dataloader:\n",
    "                # 正常传播\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                #labels = batch['labels'].to(device)\n",
    "                ids=batch['ids'].to(device)\n",
    "                print(ids)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                \n",
    "\n",
    "                # 获取预测结果\n",
    "                preds = outputs.argmax(1).to(device)\n",
    "                results=torch.cat((results,preds),dim=0)#预测的结果\n",
    "                ids_all=torch.cat((ids_all,ids),dim=0)\n",
    "\n",
    "            stacked_saved=torch.stack(\n",
    "                        (\n",
    "                            torch.as_tensor(list(map(int,ids_all))).to(device),\n",
    "                            torch.as_tensor(list(map(int,results))).to(device)#预测结果\n",
    "                        )\n",
    "                    )\n",
    "            print(ids_all)\n",
    "            stacked_saved=stacked_saved.t()\n",
    "            save_to_csv(stacked_saved,tag)\n",
    "\n",
    "        print(\"--------------dev完成-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34a417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T03:57:06.510841Z",
     "iopub.status.busy": "2023-01-09T03:57:06.510208Z",
     "iopub.status.idle": "2023-01-09T03:57:18.970162Z",
     "shell.execute_reply": "2023-01-09T03:57:18.969088Z",
     "shell.execute_reply.started": "2023-01-09T03:57:06.510785Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.18951,
   "end_time": "2023-01-09T04:26:11.568832",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-09T04:25:59.379322",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
